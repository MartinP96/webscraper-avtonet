import requests
from bs4 import BeautifulSoup
import csv
import math
from webscraper_avtonet.Scraper_Articles import ArticleList, ArticleInstance
import sys

'''
 File name: HTMLScraperAvtonet.py
 Author: Martin Porenta
 Date: 28.03.2022 
 
 Description: This file contains basic object for retriving html files from Avtonet, and scraping article
 data from recieved HTML files.
 
 Content:
    HTMLScraper_avtonet class (main class for scraping data from Avtonet)
    
 Additional Requirements:
    BeautifulSoup4 installed
    
'''

'''
    HTML Avtonet Motorcycle scraper class 
'''
class HTMLScraper_avtonet:

    # Variables
    _headers = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "accept-encoding": "gzip, deflate, br",
        "accept-language": "en-US,en;q=0.8",
        "upgrade-insecure-requests": "1"
    }

    _oblika_enum = {
        "sport": "6001",
        "naked": "6005",
        "enduro": "6002"
    }

    _filter_file_path = ""
    _search_filter = []
    _search_url = []

    """
        Class constructor: Reads search filter to dictionary and Generates 
        avtonet search url string

        Inputs:
            filter_file_path: path to scraper filter csv file
    """
    def __init__(self, filter_file_path):

        # Read search filter to dictionary
        _filter_file_path = filter_file_path
        try:
            with open(filter_file_path, 'r', encoding='UTF8', newline='') as f:
                filter_file_reader = csv.DictReader(f, delimiter=';')
                for i in filter_file_reader:
                    self._search_filter.append(i)
                    self._search_url.append(self._generate_url(i))  # Generate Avotnet search URL string list
        except:
            print("Failure: opening configuration filter")
            sys.exit()
    """
        Private filter data method: Filters scraped data according to search filter
        Inputs: 
            articles_list: list of articles to be filtered
            search_filter: search filter for filtering data
        Return: 
            filtered_article
    """
    def _filter_data(self, articles_list, search_filter):
        filtered_articles = ArticleList(articles_list.filter_name)
        for instance in articles_list.list:
            # Filter by model
            if (search_filter["model"].lower()) in instance.name.lower():
                filtered_articles.append_list(instance)
        return filtered_articles

    """
        Main public web scraper method: Public mehod to call web scraper
        Inputs: /
        Return: 
            result: all scraped data from Avtonet 
    """
    def scrape_data(self):
        result = []
        for i_filter, i_url in zip(self._search_filter, self._search_url):
            tmp = self._scrape_page(i_filter, i_url)
            result.append(tmp)

        return result

    """
        Private scrape_page method: 
        Mehod retrieves HTML from Avtonet server, parse it with BeutifulSoup library and extract relevant 
        data from parsed HTML
        
        Inputs: 
            search_filter: search filter
            url: avonet search url string
            
        Return: 
            filtered_articles
    """
    def _scrape_page(self, search_filter, url):

        # Get webpage (base page)
        response = requests.get(url, allow_redirects=False, headers=self._headers)

        # Parse HTML
        soup = BeautifulSoup(response.text, "html.parser")
        article_divs = soup.find_all("div", {"class": "row bg-white position-relative GO-Results-Row GO-Shadow-B"})

        # Check number of found articles
        num_of_articles_str = soup.find("div", {"class": "row GO-ResultsMenuBoxnaziv GO-Rounded-T justify-content-center"}).text

        # Extract number of found articles from string
        res = [int(i) for i in num_of_articles_str.split() if i.isdigit()]
        num_of_articles = res[0]
        num_per_page = 48
        num_of_pages = math.ceil(num_of_articles / num_per_page)

        # Define list of articles
        articles = ArticleList(search_filter["ime_filtra"])
        #articles_list = []

        # Loop over html divs and parse article data to objects
        instance = 1
        for page in range(1, num_of_pages + 1):
            if page > 1:
                search_url_second_page = url + str(page)
                response = requests.get(search_url_second_page, allow_redirects=False, headers=self._headers)
                # Parse HTML
                soup = BeautifulSoup(response.text, "html.parser")
                article_divs = soup.find_all("div", {"class": "row bg-white position-relative GO-Results-Row GO-Shadow-B"})

            for div in article_divs:
                current_article = ArticleInstance()
                current_article.instance = instance

                # Parse URL from div
                a_url = div.find("a", href=True)
                url_tmp = a_url['href']
                current_article.url = url_tmp.replace("..", "https://www.avto.net/")

                # Get ID from URL
                current_article.id = url_tmp[url_tmp.find("id=") + 3:url_tmp.find("&display")]

                # Parse name from div
                div_name = div.find_all("div", {
                    "class": "GO-Results-Naziv bg-dark px-3 py-2 font-weight-bold text-truncate text-white text-decoration-none"})
                span_name = div_name[0].find("span")
                current_article.name = span_name.text

                # Parse price from div
                div_price = div.find("div", {"class": "GO-Results-Price-TXT-Regular"})
                # currentArticle.price = div_price.text
                current_article.price = div_price.text.replace('Â€', '').replace(' ', '')

                # Parse year of make and kilometers from div
                div_additional_data = div.find("div", {"class": "col-auto text-truncate py-3 GO-Results-Data"})
                # Year of make
                td_year = div_additional_data.find("td", {"class": "w-75 pl-3"})
                current_article.year = td_year.text
                # Kilometers
                td_kilometers = div_additional_data.find_all("td", {"class": "pl-3"})
                current_article.kilometers = ""
                for item in td_kilometers:
                    if len(item["class"]) == 1:
                        current_article.kilometers = item.text
                        break
                # Save article object to list of articles
                articles.append_list(current_article)
                instance += 1

        # Filter  scraped data
        filtered_articles = self._filter_data(articles, search_filter)

        # return articles, filtered_articles
        return filtered_articles

    """
        Private generate_url method: 
        Method generates Avtonet search string url

        Inputs: 
            search_filter: search filter

        Return: 
            generated url string
    """
    def _generate_url(self, search_filter):
        return "https://www.avto.net/Ads/results.asp?znamka=" + search_filter["znamka"] + "&model=" + \
                  search_filter["model"] + "&modelID=" + "&tip=" + "&znamka2=" + "&model2=" + "&tip2=" + "&znamka3=" + \
                  "&model3=" + "&tip3=" + "&cenaMin=" + search_filter["cenaMin"] + "&cenaMax=" + search_filter[
                      "cenaMax"] + \
                  "&letnikMin=" + search_filter["letnikMin"] + "&letnikMax=" + search_filter["letnikMax"] + \
                  "&bencin=0""&starost2=999""&oblika=" + self._oblika_enum[search_filter["oblika"]] + "&ccmMin=" \
                  + search_filter["ccmMin"] + \
                  "&ccmMax=" + search_filter["ccmMax"] + "&mocMin=" + search_filter["mocMin"] + "&mocMax=" + \
                  search_filter["mocMax"] + "&kmMin=0" + "&kmMax=9999999" + "&kwMin=0" + "&kwMax=999" + "&motortakt=0" + \
                  "&motorvalji=0" + "&lokacija=0" + "&sirina=" + "&dolzina=" + "&dolzinaMIN=" + "&dolzinaMAX=" + \
                  "&nosilnostMIN=" + "&nosilnostMAX=" + "&lezisc=" + "&presek=" + "&premer=" + "&col=" + "&vijakov=" + \
                  "&EToznaka=" + "&vozilo=" + "&airbag=" + "&barva=" + "&barvaint=" + "&EQ1=1000000000" + "&EQ2=1000000000" \
                  + "&EQ3=1000000000" + "&EQ4=100000000" + "&EQ5=1000000000" + "&EQ6=1000000000" + "&EQ7=1110100120" + \
                  "&EQ8=1010000006" + "&EQ9=1000000000" + "&KAT=1060000000" + "&PIA=" + "&PIAzero=" + "&PSLO=" + \
                  "&akcija=" + "&paketgarancije=" + "&broker=" + "&prikazkategorije=" + "&kategorija=61000" + \
                  "&ONLvid=" + "&ONLnak=" + "&zaloga=10" + "&arhiv=" + "&presort=" + "&tipsort=" + "&stran="


